{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/raghr010/anaconda/envs/tensorflow/lib/python2.7/site-packages/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "\n",
    "MIN_LAT=40.7\n",
    "MAX_LAT=40.81\n",
    "MIN_LONG=-74\n",
    "MAX_LONG=-73.75\n",
    "LAT_BUCKETS=40\n",
    "LONG_BUCKETS=40\n",
    "BATCH_SIZE = 2000\n",
    "TRAIN_EPOCHS = 4\n",
    "\n",
    "TEST_EPOCHS = 1\n",
    "TEST_EXAMPLE_SIZE=20\n",
    "\n",
    "columns = {'vendor_id' : 0,\n",
    "        'passenger_count' :1,\n",
    "        'pickup_longitude' : 2,\n",
    "        'pickup_latitude' : 3,\n",
    "        'dropoff_longitude' : 4,\n",
    "        'dropoff_latitude' : 5,\n",
    "        'store_and_fwd_flag' : 6,\n",
    "        'trip_duration' : 7,\n",
    "        'month' : 8,\n",
    "        'date' : 9,\n",
    "        'hour' : 10,\n",
    "        'weekday' : 11,\n",
    "        'week_of_the_year' : 12,\n",
    "          'distance' : 13}\n",
    "\n",
    "def normalize_column(col):\n",
    "    return (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "float_columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "# Real valued columns\n",
    "#passenger_count    = tf.feature_column.indicator_column(normalize_column(tf.feature_column.numeric_column(\"passenger_count\", dtype=tf.int32)))\n",
    "\n",
    "# Create 20 bins for latitude, logitude and create 2 embedding column\n",
    "pickup_latitude = tf.feature_column.numeric_column(\"pickup_latitude\", dtype=tf.float64)\n",
    "pickup_latitude_feature = tf.feature_column.bucketized_column(\n",
    "    source_column=pickup_latitude,boundaries = list(np.arange( MIN_LAT, MAX_LAT, (MAX_LAT-MIN_LAT)/LAT_BUCKETS)))\n",
    "\n",
    "pickup_longitude = tf.feature_column.numeric_column(\"pickup_longitude\", dtype=tf.float64 )\n",
    "pickup_longitude_feature = tf.feature_column.bucketized_column(\n",
    "    source_column=pickup_longitude,boundaries = list(np.arange(MIN_LONG, MAX_LONG, (MAX_LONG-MIN_LONG)/LAT_BUCKETS)))\n",
    "\n",
    "\n",
    "dropoff_latitude = tf.feature_column.numeric_column(\"dropoff_latitude\", dtype=tf.float64)\n",
    "dropoff_latitude_feature = tf.feature_column.bucketized_column(\n",
    "    source_column=dropoff_latitude,boundaries = list(np.arange( MIN_LAT, MAX_LAT, (MAX_LAT-MIN_LAT)/LAT_BUCKETS)))\n",
    "\n",
    "dropoff_longitude = tf.feature_column.numeric_column(\"dropoff_longitude\", dtype=tf.float64)\n",
    "dropoff_longitude_feature = tf.feature_column.bucketized_column(\n",
    "    source_column=dropoff_longitude,boundaries = list(np.arange(MIN_LONG, MAX_LONG, (MAX_LONG-MIN_LONG)/LAT_BUCKETS)))\n",
    "\n",
    "\n",
    "distance = tf.feature_column.numeric_column(\"distance\", dtype=tf.float64)\n",
    "\n",
    "pickup_lat_x_long = tf.feature_column.embedding_column(\n",
    "    tf.feature_column.crossed_column(\n",
    "        keys=[pickup_latitude_feature, pickup_longitude_feature],\n",
    "        hash_bucket_size=400\n",
    "    ),\n",
    "    dimension=20\n",
    ")\n",
    "\n",
    "\n",
    "# Direct columns from file\n",
    "vendor = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('vendor_id', [1,2], dtype=tf.int32))\n",
    "\n",
    "store_and_fwd_flag =tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('store_and_fwd_flag', [1,0], dtype=tf.int32))\n",
    "\n",
    "# Date columns\n",
    "month = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('month', range(1,12), dtype=tf.int32))\n",
    "date = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('date', range(1,31), dtype=tf.int32))\n",
    "hour = tf.feature_column.categorical_column_with_vocabulary_list('hour', range(0,23), dtype=tf.int32)\n",
    "\n",
    "\n",
    "weekday = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('weekday', range(0,6), dtype=tf.int32))\n",
    "week_of_the_year = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('week_of_the_year', range(1,52), dtype=tf.int32))\n",
    "\n",
    "# month_date_hour = tf.feature_column.indicator_column(\n",
    "#     tf.feature_column.crossed_column(\n",
    "#         keys=[month, date, hour],\n",
    "#         hash_bucket_size=8928\n",
    "#     )\n",
    "# )\n",
    "#\n",
    "# weekday_hour = tf.feature_column.indicator_column(\n",
    "#     tf.feature_column.crossed_column(\n",
    "#         keys=[weekday, hour],\n",
    "#         hash_bucket_size=364\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "feature_columns = { pickup_lat_x_long,\n",
    "                   vendor, weekday, hour, week_of_the_year, week_of_the_year, distance}\n",
    "\n",
    "print feature_columns\n",
    "\n",
    "def create_training_and_test_data(file_name):\n",
    "\n",
    "    df = pd.read_csv(file_name, header=0)\n",
    "\n",
    "    def pickup_weekday(row):\n",
    "        return datetime.date(row['year'], row['month'], row['date']).weekday()\n",
    "\n",
    "    def week_of_year(row):\n",
    "        return datetime.date(row['year'], row['month'], row['date']).isocalendar()[1]\n",
    "\n",
    "    def store_and_forward(row):\n",
    "        return 1 if row['store_and_fwd_flag'] == 'Y' else 0\n",
    "\n",
    "    df['month'] = df['pickup_datetime'].str.split('-').str[1].astype(np.int32)\n",
    "    df['year'] = df['pickup_datetime'].str.split('-').str[0].astype(np.int32)\n",
    "    df['date'] = df['pickup_datetime'].str.split('-').str[2].str.split(' ').str[0].astype(np.int32)\n",
    "    df['hour'] = df['pickup_datetime'].str.split(' ').str[1].str.split(':').str[0].astype(np.int32)\n",
    "    df['store_and_fwd_flag'] = df.apply(store_and_forward, axis=1)\n",
    "    df['weekday'] = df.apply(pickup_weekday, axis=1)\n",
    "\n",
    "    df['week_of_the_year'] = df.apply(week_of_year, axis=1)\n",
    "\n",
    "    del df['pickup_datetime']\n",
    "    del df['dropoff_datetime']\n",
    "\n",
    "    del df['year']\n",
    "\n",
    "    del df['id']\n",
    "\n",
    "    np_array = df.values\n",
    "    np.random.shuffle(np_array)\n",
    "\n",
    "    np.save('training_data.npy', np_array[:TEST_EXAMPLE_SIZE])\n",
    "    np.save('testing_data.npy', np_array[TEST_EXAMPLE_SIZE:])\n",
    "\n",
    "    return df\n",
    "\n",
    "def make_input_fn(file_name, test=False):\n",
    "\n",
    "    df = pd.read_csv(file_name, header=0)\n",
    "\n",
    "    def pickup_weekday(row):\n",
    "        return datetime.date(row['year'], row['month'], row['date']).weekday()\n",
    "\n",
    "    def week_of_year(row):\n",
    "        return datetime.date(row['year'], row['month'], row['date']).isocalendar()[1]\n",
    "\n",
    "    def store_and_forward(row):\n",
    "        return 1 if row['store_and_fwd_flag'] == 'Y' else 0\n",
    "\n",
    "    df['month'] = df['pickup_datetime'].str.split('-').str[1].astype(np.int32)\n",
    "    df['year'] = df['pickup_datetime'].str.split('-').str[0].astype(np.int32)\n",
    "    df['date'] = df['pickup_datetime'].str.split('-').str[2].str.split(' ').str[0].astype(np.int32)\n",
    "    df['hour'] = df['pickup_datetime'].str.split(' ').str[1].str.split(':').str[0].astype(np.int32)\n",
    "    df['store_and_fwd_flag'] = df.apply(store_and_forward, axis=1)\n",
    "    df['weekday'] = df.apply(pickup_weekday, axis=1)\n",
    "\n",
    "    df['week_of_the_year'] = df.apply(week_of_year, axis=1)\n",
    "\n",
    "    del df['pickup_datetime']\n",
    "    del df['dropoff_datetime']\n",
    "\n",
    "    del df['year']\n",
    "\n",
    "    del df['id']\n",
    "\n",
    "    df_train = df[: -1 * TEST_EXAMPLE_SIZE]\n",
    "    df_test = df[-1 * TEST_EXAMPLE_SIZE : ]\n",
    "\n",
    "    # Create tensorflow input fn\n",
    "    def train_input_fn():\n",
    "        # Wrap the useful features in an array\n",
    "        useful_fueatures = [\n",
    "            np.array(df_train[\"month\"].values, dtype=np.int32),\n",
    "            np.array(df_train[\"date\"].values, dtype=np.int32),\n",
    "            np.array(df_train[\"hour\"].values, dtype=np.int32),\n",
    "            np.array(df_train[\"weekday\"].values, dtype=np.int32),\n",
    "            np.array(df_train[\"week_of_the_year\"].values, dtype=np.int32),\n",
    "            np.array(df_train['vendor_id'], dtype=np.int32),\n",
    "            np.array(df_train['passenger_count'], dtype=np.int32),\n",
    "            np.array(df_train['pickup_longitude'], dtype=np.float32),\n",
    "            np.array(df_train['pickup_latitude'], dtype=np.float32),\n",
    "            np.array(df_train['dropoff_longitude'], dtype=np.float32),\n",
    "            np.array(df_train['dropoff_latitude'], dtype=np.float32),\n",
    "            np.array(df_train['store_and_fwd_flag'], dtype=np.int32),\n",
    "            np.array(df_train['trip_duration'], dtype=np.int32)\n",
    "        ]\n",
    "\n",
    "        # Ugly, but creates all the slice input producers for all the features selected\n",
    "        month, date, hour, \\\n",
    "        weekday, week_of_the_year, vendor_id, \\\n",
    "        passenger_count, pickup_longitude, pickup_latitude, dropoff_longitude,dropoff_latitude, \\\n",
    "        store_and_fwd_flag, trip_duration = tf.train.slice_input_producer(\n",
    "            tensor_list=useful_fueatures,\n",
    "            num_epochs=TRAIN_EPOCHS,\n",
    "            shuffle=True,\n",
    "            capacity=BATCH_SIZE * 5\n",
    "        )\n",
    "\n",
    "        # Created a dict out of sliced input producers\n",
    "        dataset_dict = dict(\n",
    "            month=month,\n",
    "            date=date,\n",
    "            hour=hour,\n",
    "            weekday=weekday,\n",
    "            week_of_the_year=week_of_the_year,\n",
    "            vendor_id=vendor_id,\n",
    "            passenger_count=passenger_count,\n",
    "            pickup_longitude=pickup_longitude,\n",
    "            pickup_latitude=pickup_latitude,\n",
    "            dropoff_longitude=dropoff_longitude,\n",
    "            dropoff_latitude=dropoff_latitude,\n",
    "            store_and_fwd_flag=store_and_fwd_flag,\n",
    "            trip_duration=trip_duration\n",
    "        )\n",
    "\n",
    "        # Creates a batched dictionary that holds a queue that loads the data\n",
    "        # while the training is happening. Multithreading.\n",
    "        batch_dict = tf.train.batch(\n",
    "            dataset_dict,\n",
    "            BATCH_SIZE,\n",
    "            num_threads=10,\n",
    "            capacity=BATCH_SIZE * 5,\n",
    "            enqueue_many=False,\n",
    "            dynamic_pad=False,\n",
    "            allow_smaller_final_batch=True,\n",
    "            shared_name=None,\n",
    "            name=None\n",
    "        )\n",
    "\n",
    "        # The labels need to be returned separately\n",
    "        batch_labels = batch_dict.pop('trip_duration')\n",
    "        return batch_dict, tf.reshape(batch_labels, [-1, 1])\n",
    "\n",
    "    # Create tensorflow input fn\n",
    "    def test_input_fn():\n",
    "        # Wrap the useful features in an array\n",
    "        useful_fueatures = [\n",
    "            np.array(df_test[\"month\"].values, dtype=np.int32),\n",
    "            np.array(df_test[\"date\"].values, dtype=np.int32),\n",
    "            np.array(df_test[\"hour\"].values, dtype=np.int32),\n",
    "            np.array(df_test[\"weekday\"].values, dtype=np.int32),\n",
    "            np.array(df_test[\"week_of_the_year\"].values, dtype=np.int32),\n",
    "            np.array(df_test['vendor_id'], dtype=np.int32),\n",
    "            np.array(df_test['passenger_count'], dtype=np.int32),\n",
    "            np.array(df_test['pickup_longitude'], dtype=np.float32),\n",
    "            np.array(df_test['pickup_latitude'], dtype=np.float32),\n",
    "            np.array(df_test['dropoff_longitude'], dtype=np.float32),\n",
    "            np.array(df_test['dropoff_latitude'], dtype=np.float32),\n",
    "            np.array(df_test['store_and_fwd_flag'], dtype=np.int32),\n",
    "            np.array(df_test['trip_duration'], dtype=np.int32)\n",
    "        ]\n",
    "\n",
    "        # Ugly, but creates all the slice input producers for all the features selected\n",
    "        month, date, hour, \\\n",
    "        weekday, week_of_the_year, vendor_id, \\\n",
    "        passenger_count, pickup_longitude, pickup_latitude, dropoff_longitude,dropoff_latitude, \\\n",
    "        store_and_fwd_flag, trip_duration = tf.train.slice_input_producer(\n",
    "            tensor_list=useful_fueatures,\n",
    "            num_epochs=TEST_EPOCHS,\n",
    "            shuffle=True,\n",
    "            capacity=BATCH_SIZE * 5\n",
    "        )\n",
    "\n",
    "        # Created a dict out of sliced input producers\n",
    "        dataset_dict = dict(\n",
    "            month=month,\n",
    "            date=date,\n",
    "            hour=hour,\n",
    "            weekday=weekday,\n",
    "            week_of_the_year=week_of_the_year,\n",
    "            vendor_id=vendor_id,\n",
    "            passenger_count=passenger_count,\n",
    "            pickup_longitude=pickup_longitude,\n",
    "            pickup_latitude=pickup_latitude,\n",
    "            dropoff_longitude=dropoff_longitude,\n",
    "            dropoff_latitude=dropoff_latitude,\n",
    "            store_and_fwd_flag=store_and_fwd_flag,\n",
    "            trip_duration=trip_duration\n",
    "        )\n",
    "\n",
    "        # Creates a batched dictionary that holds a queue that loads the data\n",
    "        # while the training is happening. Multithreading.\n",
    "        batch_dict = tf.train.batch(\n",
    "            dataset_dict,\n",
    "            BATCH_SIZE,\n",
    "            num_threads=10,\n",
    "            capacity=BATCH_SIZE * 5,\n",
    "            enqueue_many=False,\n",
    "            dynamic_pad=False,\n",
    "            allow_smaller_final_batch=True,\n",
    "            shared_name=None,\n",
    "            name=None\n",
    "        )\n",
    "\n",
    "        # The labels need to be returned separately\n",
    "        batch_labels = batch_dict.pop('trip_duration')\n",
    "        return batch_dict, tf.reshape(batch_labels, [-1, 1])\n",
    "\n",
    "    return train_input_fn, test_input_fn\n",
    "\n",
    "\n",
    "def make_model(features, labels, mode, params, config):\n",
    "    # Creates the input layer starting from the feature definitions of above\n",
    "    input_layer = tf.feature_column.input_layer(\n",
    "        features=features,\n",
    "        feature_columns=feature_columns\n",
    "    )\n",
    "\n",
    "    # Get the global step\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "    # First dense layer or the neural net\n",
    "    x = tf.layers.dense(\n",
    "        inputs=input_layer,\n",
    "        units=512,\n",
    "        activation=tf.nn.relu,\n",
    "        name=\"fisrt_fully_connected_layer\"\n",
    "    )\n",
    "\n",
    "    # Adding dropout to lessen chances of overfitting\n",
    "    x = tf.layers.dropout(\n",
    "        inputs=x,\n",
    "        name=\"first_dropout\"\n",
    "    )\n",
    "\n",
    "    # Second dense layer\n",
    "    x = tf.layers.dense(\n",
    "        inputs=x,\n",
    "        units=128,\n",
    "        activation=tf.nn.relu,\n",
    "        name=\"second_fully_connected_layer\"\n",
    "    )\n",
    "\n",
    "    # Third and final deep layer of the neural net\n",
    "    x = tf.layers.dense(\n",
    "        inputs=x,\n",
    "        units=16,\n",
    "        activation=tf.nn.relu,\n",
    "        name=\"third_fully_connected_layer\"\n",
    "    )\n",
    "\n",
    "    # Linear output neuron that combine the output of the neural net\n",
    "    predictions = tf.contrib.layers.fully_connected(\n",
    "        inputs=x,\n",
    "        num_outputs=1\n",
    "    )\n",
    "\n",
    "    # Loss is defined as the L1 distance since it is less sensitive to outliers\n",
    "    loss = tf.losses.absolute_difference(\n",
    "        labels=labels,\n",
    "        predictions=predictions\n",
    "    )\n",
    "\n",
    "    # Export the loss to tensorboard\n",
    "    tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "    # Using ADAgrad Momentum Optimizer since it provides quite some advance features and\n",
    "    # turns out to be very stable\n",
    "    optimizer = tf.train.AdamOptimizer(\n",
    "        learning_rate=params.learning_rate,\n",
    "    )\n",
    "\n",
    "    # Out train op in the tensorflow graph. Computing this also increases our global_step\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Finally, wrap the tensor defined above in the format Tensorflow expects\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main file\n",
    "def main(_):\n",
    "    #create_training_and_test_data('train1.csv')\n",
    "    train_input_fn, test_input_fn = make_input_fn('train.csv')\n",
    "\n",
    "\n",
    "    # Creates hyperparams\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        learning_rate=.1,\n",
    "    )\n",
    "\n",
    "    config = tf.ConfigProto(\n",
    "        # allow_soft_placement=True,\n",
    "        # log_device_placement=True\n",
    "    )\n",
    "    # Turns on JIT Compilation through XLA for boosting performance. If crashes disable this\n",
    "    # config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "    trainingConfig = tf.contrib.learn.RunConfig(\n",
    "        # log_device_placement=True,\n",
    "        save_summary_steps=500,\n",
    "        save_checkpoints_steps=500,\n",
    "        # Creates model dir (need to change this)\n",
    "        model_dir=(\"/tmp/\"),\n",
    "        session_config=config\n",
    "    )\n",
    "\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=make_model,\n",
    "        params=hparams,\n",
    "        config=trainingConfig\n",
    "    )\n",
    "\n",
    "    # Finally, perform the training (VERY VERY LONG!)\n",
    "    estimator.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=TRAIN_EPOCHS,\n",
    "    )\n",
    "\n",
    "    print estimator.evaluate(input_fn=test_input_fn, steps=TEST_EPOCHS)\n",
    "\n",
    "\n",
    "# Run the main\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run(main)\n",
    "    #create_training_and_test_data('train1.csv')\n",
    "\n",
    "    #df = np.load('training_data.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
